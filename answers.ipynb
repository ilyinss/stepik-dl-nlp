{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('./stepik-dl-nlp')\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
    "    vectorize_texts, SparseFeaturesDataset\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Закон Ципфа  \n",
    "$$f(rank, Z(s, N)) = \\frac{1}{Z(s, N)*rank^{s}}$$  \n",
    "\n",
    "## Z-константа  \n",
    "$$Z(s, N) = \\sum\\limits_{i=1}^{N}i^{-s}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь - {слово: ранг встречаемости}\n",
    "vocabl = {'a':1, 'b':2, 'c':3}\n",
    "# затухание\n",
    "s = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zipf(entry_dict, s):\n",
    "    n = len(entry_dict)\n",
    "    # z - нормировочная константа\n",
    "    z = sum([i**(-s) for i in range(1, n+1)])\n",
    "    zipf_l = []\n",
    "    for i in entry_dict.keys():    \n",
    "        zipf_l.append((z**(-1))*(1/entry_dict.get(i)**s))\n",
    "    return(zipf_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7346938775510203, 0.18367346938775508, 0.08163265306122448]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_zipf(vocabl, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Написать регулярку\n",
    "[stepik question link](https://stepik.org/lesson/256724/step/3?unit=237035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы предлагаем вам собрать регулярное выражение из \"деталей\" так, чтобы оно выделяло в отдельные токены знаки препинания, числа и слова.\n",
    "\n",
    "А именно:\n",
    "\n",
    "Числа с плавающей точкой вида 123.23 выделяются в один токен. Десятичным разделителем может быть точка или запятая.\n",
    "Число может быть отрицательным: иметь знак -123.4−123.4\n",
    "Целой части числа может вовсе не быть: последовательности  -0.15−0.15  и  -.15−.15   означают одно и то же число.\n",
    "При этом числа с нулевой дробной частью не допускаются:  строка \"12345.12345.\" будет разделена на два токена \"12345\" и \".\"\n",
    "Идущие подряд знаки препинания выделяются каждый в отдельный токен.\n",
    "Наконец множество букв в словах ограничивается только кириллическим алфавитом (33 буквы, включая букву ё).\n",
    "Обратите внимание, что в результате токенизации не должно получаться пустых токенов.\n",
    "\n",
    "Вы можете использовать следующие тесты для отладки своего регулярного выражения:\n",
    "\n",
    "|  Текст  |  Результат  |  \n",
    "|---------|-------------|\n",
    "|Контактный телефон: 123123.  |контактный телефон : 123123 .|\n",
    "|Что-нибудь надо придумать.  |что - нибудь надо придумать .|\n",
    "|Значение числа Е=2.7182.  |значение числа е = 2.7182 .|\n",
    "|Демон123, как тебя зовут в реале?  |демон 123 , как тебя зовут в реале ?|\n",
    "|-1-.15=-1.15  |-1 -.15 = -1.15|\n",
    "|- 1 - .15 = -1.15  |- 1 - .15 = -1.15|\n",
    "|Какого ;%:?* тут происходит?  |какого ; % : ? * тут происходит ?|\n",
    " \n",
    "\n",
    "Детали \"конструктора\", из которых можно собрать решение задачи:  \n",
    "\n",
    "- [а-яё]+          // ненулевая последовательность любых букв русского алфавита.\n",
    "- -?\\d*[.,]?\\d+   // возможно знак, возможная целая часть числа, возможно, десятичный знак и остальная часть числа.\n",
    "- \\S                // любой символ, кроме разделителей (пробелов, переносов строк)\n",
    "- |     //  | отвечает за выбор из двух паттернов, например: [а-яё]+|\\d последовательность букв или одна цифра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# модифицированное выражение\n",
    "TOKENIZE_RE = re.compile(r'[а-яё]+|-?\\d*[.,]?\\d+|\\S', re.I)\n",
    "\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print(' '.join(tokenize(line.strip().lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Мама', 'мыла', 'раму', '.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "tokenize('Мама мыла раму.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подсчитать частотность токенов\n",
    "[stepik question link](https://stepik.org/lesson/256724/step/5?unit=237035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дана следующая коллекция текстов. Постройте словарь (отображение из строкового представления токенов в их номера) и вектор весов (DF).\n",
    "\n",
    "$$DF(w) = \\frac{DocCount(w, c)}{Size(c)}$$\n",
    "\n",
    "DF(w) -- частота слова w в коллекции c (отношение количества документов, в которых слово используется, к общему количеству документов).\n",
    "\n",
    "> Казнить нельзя, помиловать. Нельзя наказывать.\n",
    "\n",
    "> Казнить, нельзя помиловать. Нельзя освободить.\n",
    "\n",
    "> Нельзя не помиловать.\n",
    "\n",
    "> Обязательно освободить.\n",
    "\n",
    "При токенизации используйте регулярное выражение из семинара: [\\w\\d]+. После токенизации все токены нужно привести к нижнему регистру. Фильтрацию по частоте не использовать.\n",
    "\n",
    "Ответ запишите в две строки:\n",
    "\n",
    "в первой строке - содержимое словаря - список уникальных токенов через пробел в порядке возрастания частоты встречаемости. При одинаковой частоте сортировать по алфавиту.\n",
    "во второй строке - список весов (DF) токенов, округлённых до 2 знака после запятой и разделённых пробелами, в том же порядке, что и токены в первой строке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Казнить нельзя, помиловать. Нельзя наказывать.',\n",
    "    'Казнить, нельзя помиловать. Нельзя освободить.',\n",
    "    'Нельзя не помиловать.',\n",
    "    'Обязательно освободить.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZE_RE = re.compile(r'[\\w\\d]+', re.I)\n",
    "\n",
    "corpus = [x.lower() for x in corpus]\n",
    "\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt)\n",
    "\n",
    "corpus_tokens = [tokenize(x) for x in corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_tokens = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'],\n",
       " ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'],\n",
       " ['нельзя', 'не', 'помиловать'],\n",
       " ['обязательно', 'освободить']]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DF = 1\n",
    "MIN_COUNT = 1\n",
    "vocabulary, word_doc_freq = build_vocabulary(corpus_tokens, max_doc_freq=MAX_DF, min_count=MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных токенов 7\n",
      "[('нельзя', 0), ('помиловать', 1), ('казнить', 2), ('освободить', 3), ('наказывать', 4), ('не', 5), ('обязательно', 6)]\n"
     ]
    }
   ],
   "source": [
    "UNIQUE_WORDS_N = len(vocabulary)\n",
    "print('Количество уникальных токенов', UNIQUE_WORDS_N)\n",
    "print(list(vocabulary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('нельзя', 0.75),\n",
       " ('помиловать', 0.75),\n",
       " ('казнить', 0.5),\n",
       " ('освободить', 0.5),\n",
       " ('наказывать', 0.25),\n",
       " ('не', 0.25),\n",
       " ('обязательно', 0.25)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = [(word, word_doc_freq[i]) for i, (word, _) in enumerate(vocabulary.items())]\n",
    "word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наказывать не обязательно казнить освободить нельзя помиловать\n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "answer = sorted(word_df, key = lambda x: x[1])\n",
    "answer_1 = []; \n",
    "answer_2 = [];\n",
    "\n",
    "for k, v in list(answer):\n",
    "    answer_1.append(k)\n",
    "    answer_2.append(str(v))\n",
    "    \n",
    "print(\" \".join(answer_1))\n",
    "print(\" \".join(answer_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постройте матрицу признаков для текстов\n",
    "[stepik question link]('https://stepik.org/lesson/256724/step/7?unit=237035')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте матрицу признаков для текстов с шага 5 с использованием словаря и вектора весов, полученного на шаге 5. Используйте взвешивание  \n",
    "$$lTFIDF = \\ln(TF + 1) \\cdot IDF$$  \n",
    "\n",
    "<!-- lTFIDF=ln(TF+1)⋅IDF. -->\n",
    "\n",
    "Значения признаков следует отмасштабировать так, чтобы для каждого признака его среднее значение по выборке равнялось 0, а среднеквадратичное отклонение \n",
    "\n",
    "$$1: x^{scaled}_{i} = \\frac{x_{i} - E(x)} {\\sigma(x)}$$\n",
    "\n",
    "В результате масштабирования **для каждого столбца** матрицы признаков среднее должно равняться 0, а среднеквадратичное отклонение 1.\n",
    "\n",
    "При расчёте среднеквадратического отклонения необходимо использовать скорректированную оценку $$\\sigma=\\sqrt{\\frac{\\sum_{i-1}^n(x_i - E(x))^2}{n - 1}}$$\n",
    "Чтобы получить такую оценку с помощью numpy, необходимо передать параметр `ddof=1`: \n",
    "\n",
    "```\n",
    "feature_matrix = np.zeros((num_docs, num_feats))\n",
    "feats_std = feature_matrix.std(0, ddof=1)\n",
    "```\n",
    "\n",
    "Ответ отформатируйте так, чтобы на каждой строке были признаки одного документа. Порядок столбцов должен соответствовать порядку слов в словаре (как в ответе на шаге 5, по возрастанию df). Столбцы разделяйте одним пробелом. В качестве разделителя целой и дробной части используйте точку или запятую. Округлять значения не обязательно. Решение, при проверке, автоматически округлится до двух знаков. Метод округления - либо \"математический\", либо свойственный Python rounding half to even strategy, если интересно, посмотрите IEEE 754.\n",
    "\n",
    "Пример ответа для первых двух документов (до полного ответа не хватает ещё двух строк):\n",
    "\n",
    "> 1.5  -0.5 -0.5 0.87 -0.76 0.60 0.16  \n",
    "> -0.5 -0.5 -0.5 0.87 0.18  0.60 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.50 -0.50 -0.50 0.87 -0.76 0.60 0.16\n",
      "-0.50 -0.50 -0.50 0.87 0.18 0.60 0.16\n",
      "-0.50 1.50 -0.50 -0.87 -0.76 0.29 1.04\n",
      "-0.50 -0.50 1.50 -0.87 1.34 -1.48 -1.36\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Казнить нельзя, помиловать. Нельзя наказывать.',\n",
    "    'Казнить, нельзя помиловать. Нельзя освободить.',\n",
    "    'Нельзя не помиловать.',\n",
    "    'Обязательно освободить.']\n",
    "\n",
    "#Получаем счетчики слов\n",
    "TF = CountVectorizer().fit_transform(corpus)\n",
    "\n",
    "#Строим IDF. К сожалению, в этом задании нам нужно только vectorizer.idf_\n",
    "#Для стандартных случаев на этой строке все вычисления и заканчиваются.\n",
    "#Обычно  TFIDF = vectorizer.fit_transform(corpus)\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, use_idf=True)\n",
    "vectorizer.fit_transform(corpus)\n",
    "\n",
    "## из IDF  в DF\n",
    "word_doc_freq = 1/np.exp(vectorizer.idf_ - 1)\n",
    "\n",
    "#TF нормируем и сглаживаем логарифмом (требование задания)\n",
    "TFIDF = np.log(TF/TF.sum(axis=1)+1) / word_doc_freq \n",
    "\n",
    "#Масштабируем признаки\n",
    "scaledTFIDF = StandardScaler().fit_transform(TFIDF)\n",
    "\n",
    "#Домножаем на np.sqrt((4-1)/4) для перевода из DDOF(0) в DDOF(1) для 4 текстов\n",
    "#(требование задания) \n",
    "scaledTFIDF *= np.sqrt(3/4)\n",
    "\n",
    "#Вывод в порядке возрастания DF\n",
    "for l in scaledTFIDF[:,np.argsort(word_doc_freq)]:\n",
    "    print (\" \".join([ \"%.2f\" % d for d in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оцените количество памяти  \n",
    "[stepik question link]('https://stepik.org/lesson/256724/step/11?unit=237035')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы занимаетесь тематической классификацией текстов (например, сортируете объявления пользователей по категориям). У вас есть коллекция из 100000 текстов, в которой содержится 10000 уникальных токенов (это размер словаря). Вы хотите построить матрицу признаков для текстов в вашем датасете, чтобы затем обучить логистическую регрессию.  \n",
    "\n",
    "Построив матрицу, вы обнаруживаете, что 99.5% значений матрицы - нулевые. \n",
    "\n",
    "Если бы вы хранили датасет в плотной матрице (например, np.array), то вам бы потребовалось достаточно много памяти (каждое значение признака занимает 4 байта, тип np.float32):\n",
    "\n",
    "$$dense = |texts| \\times |vocab| \\times 4bytes = 4 \\cdot 10^9 bytes \\approx 3814.7Megabytes$$  \n",
    "\n",
    "(здесь мы считаем, что в мегабайте 1024 килобайт, в килобайте - 1024 байт.)\n",
    "\n",
    "Вместо этого вы решаете хранить датасет в разреженной матрице в формате COO (coordinate, scipy.sparse.coo_matrix). В этом формате для каждого ненулевого элемента хранится три числа: значение элемента, номер столбца и номер строки. Для хранения координат используется тип np.uint32, 4 байта на каждое значение.\n",
    "\n",
    "Оцените количество памяти, которое экономится при использовании разреженной матрицы для хранения датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000/100*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ:  \n",
    "\n",
    "$$sparse=∣texts∣×∣vocab∣×(4bytes+4bytes+4bytes)×0.005=6⋅10^{7} bytes$$  \n",
    " \n",
    "$$ SavedMemory=\\frac{\\text{(dense−sparse)}}{\\text{bytes in megabyte}}≈3757.5Megabytes$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00125c6e5922b1d36e973e80bd3f7977debb1f35a6d2102f4b9bdb0261afff1c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('study_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
